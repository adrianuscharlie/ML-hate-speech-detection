{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan import libraries yang dibutukan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import codecs\n",
    "factory=StemmerFactory()\n",
    "stemmer=factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan reading data hate speech tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/data.csv',encoding='latin-1')\n",
    "stopword=pd.read_csv('data/stopwordbahasa.csv',header=None)\n",
    "stopword=stopword.rename(columns={0:'stopwords'})\n",
    "kamus_alay=pd.read_csv('data/new_kamusalay.csv',header=None,encoding='latin-1')\n",
    "kamus_alay=kamus_alay.rename(columns={0:'asli',1:'baru'})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menggunakan data HS dan Abusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[['Tweet','HS','Abusive']]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming text\n",
    "def stemWord(x):\n",
    "    return stemmer.stem(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lower text\n",
    "def lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove unnecessary charracters\n",
    "def removeUnnecessary(text):\n",
    "    text=re.sub('\\n','',text) # menghapus newline\n",
    "    text=re.sub('rt','',text) # menghapus kata retweet\n",
    "    text=re.sub('user','',text) # menghapus kata user\n",
    "    text=re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) # menghapus url\n",
    "    text = re.sub('  +', ' ', text) # menghapus karakter ekstra\n",
    "    text=re.sub('x.{3} | x.{2}','',text) # menghapus pola emoji \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove non alphanumeric character\n",
    "def removeNonAlphanumeric(text):\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n",
    "    text=\"\".join(filter(lambda x: not x.isdigit(), text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove indonesian stopword\n",
    "def removeStopWord(text):\n",
    "    text = ' '.join(['' if word in stopword.stopwords.values else word for word in text.split(' ')])\n",
    "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
    "    text = text.strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize alay\n",
    "alay=dict(zip(kamus_alay['asli'],kamus_alay['baru']))\n",
    "def normalizeAlay(text):\n",
    "    return ' '.join([alay[word] if word in alay else word for word in text.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessing(sentences):\n",
    "    sentences=lowercase(sentences)\n",
    "    sentences=removeNonAlphanumeric(sentences)\n",
    "    sentences=removeUnnecessary(sentences)\n",
    "    sentences=normalizeAlay(sentences)\n",
    "    sentences=stemWord(sentences)\n",
    "    sentences=removeStopWord(sentences)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Tweet']=data['Tweet'].apply(preProcessing)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data_clean.csv',columns=['Tweet','HS','Abusive'],index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data_clean.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data Into Train, Validation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation\n",
    "x_train,x_val,y_train,y_val=train_test_split(data['Tweet'],data['HS'],test_size=0.2,random_state=1)\n",
    "\n",
    "#Split train data into train and test\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_train,y_train,test_size=0.25,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=1000\n",
    "max_len=100\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=vocab_size,oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(x_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff0d91300876931207232d01add3156fa7c8214350996c757a3c6cebc4b3b5d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
