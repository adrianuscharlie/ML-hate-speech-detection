{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan import libraries yang dibutukan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import codecs\n",
    "factory=StemmerFactory()\n",
    "stemmer=factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan reading data hate speech tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/data.csv',encoding='latin-1')\n",
    "stopword=pd.read_csv('data/stopwordbahasa.csv',header=None)\n",
    "stopword=stopword.rename(columns={0:'stopwords'})\n",
    "kamus_alay=pd.read_csv('data/new_kamusalay.csv',header=None,encoding='latin-1')\n",
    "kamus_alay=kamus_alay.rename(columns={0:'asli',1:'baru'})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menggunakan data HS dan Abusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[['Tweet','HS','Abusive']]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming text\n",
    "def stemWord(x):\n",
    "    return stemmer.stem(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lower text\n",
    "def lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove unnecessary charracters\n",
    "def removeUnnecessary(text):\n",
    "    text=re.sub('\\n','',text) # menghapus newline\n",
    "    text=re.sub('rt','',text) # menghapus kata retweet\n",
    "    text=re.sub('user','',text) # menghapus kata user\n",
    "    text=re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) # menghapus url\n",
    "    text = re.sub('  +', ' ', text) # menghapus karakter ekstra\n",
    "    text=re.sub('x.{3} | x.{2}','',text) # menghapus pola emoji \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove non alphanumeric character\n",
    "def removeNonAlphanumeric(text):\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n",
    "    text=\"\".join(filter(lambda x: not x.isdigit(), text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove indonesian stopword\n",
    "def removeStopWord(text):\n",
    "    text = ' '.join(['' if word in stopword.stopwords.values else word for word in text.split(' ')])\n",
    "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
    "    text = text.strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize alay\n",
    "alay=dict(zip(kamus_alay['asli'],kamus_alay['baru']))\n",
    "def normalizeAlay(text):\n",
    "    return ' '.join([alay[word] if word in alay else word for word in text.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessing(sentences):\n",
    "    sentences=lowercase(sentences)\n",
    "    sentences=removeNonAlphanumeric(sentences)\n",
    "    sentences=removeUnnecessary(sentences)\n",
    "    sentences=normalizeAlay(sentences)\n",
    "    sentences=stemWord(sentences)\n",
    "    sentences=removeStopWord(sentences)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Tweet']=data['Tweet'].apply(preProcessing)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data_clean.csv',columns=['Tweet','HS','Abusive'],index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>HS</th>\n",
       "      <th>Abusive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cowok usaha lacak perhati gue lantas remeh per...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telat tau edan sarap gue gaul cigax jifla cal ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kadang pikir percaya tuhan jatuh kali kali kad...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ku tau mata sipit lihat</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kaum cebong kafir lihat dongok dungu haha</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13164</th>\n",
       "      <td>bicara ndasmu congor kate anjing</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13165</th>\n",
       "      <td>kasur enak kunyuk</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13166</th>\n",
       "      <td>hati hati bisu bosan huftxaa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13167</th>\n",
       "      <td>bom real mudah deteksi bom kubur dahsyat ledak...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13168</th>\n",
       "      <td>situ foto ya kutil onta</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13169 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Tweet  HS  Abusive\n",
       "0      cowok usaha lacak perhati gue lantas remeh per...   1        1\n",
       "1      telat tau edan sarap gue gaul cigax jifla cal ...   0        1\n",
       "2      kadang pikir percaya tuhan jatuh kali kali kad...   0        0\n",
       "3                                ku tau mata sipit lihat   0        0\n",
       "4              kaum cebong kafir lihat dongok dungu haha   1        1\n",
       "...                                                  ...  ..      ...\n",
       "13164                   bicara ndasmu congor kate anjing   1        1\n",
       "13165                                  kasur enak kunyuk   0        1\n",
       "13166                       hati hati bisu bosan huftxaa   0        0\n",
       "13167  bom real mudah deteksi bom kubur dahsyat ledak...   0        0\n",
       "13168                            situ foto ya kutil onta   1        1\n",
       "\n",
       "[13169 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('data_clean.csv')\n",
    "data.Tweet=data.Tweet.astype(str)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel('data_clean.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data Into Train, Validation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation\n",
    "x_train,x_val,y_train,y_val=train_test_split(data['Tweet'],data['HS'],test_size=0.2,random_state=1)\n",
    "\n",
    "#Split train data into train and test\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_train,y_train,test_size=0.25,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the train,val and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=1000\n",
    "max_len=100\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=vocab_size,oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences=tokenizer.texts_to_sequences(x_train)\n",
    "padded=pad_sequences(sequences=sequences,padding=padding_type,truncating=trunc_type,maxlen=max_len)\n",
    "sequences_val=tokenizer.texts_to_sequences(x_val)\n",
    "padded_val=pad_sequences(sequences=sequences_val,padding=padding_type,truncating=trunc_type,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 64)           64000     \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 100, 128)         66048     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 128)              98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 245,505\n",
      "Trainable params: 245,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, 64, input_length=max_len),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)),\n",
    "         tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "         tf.keras.layers.Dense(128,activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the loss function, optimizer and metrics for the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "247/247 [==============================] - 26s 48ms/step - loss: 0.5041 - accuracy: 0.7451 - val_loss: 0.4153 - val_accuracy: 0.8041\n",
      "Epoch 2/20\n",
      "247/247 [==============================] - 10s 42ms/step - loss: 0.3642 - accuracy: 0.8451 - val_loss: 0.4219 - val_accuracy: 0.8075\n",
      "Epoch 3/20\n",
      "247/247 [==============================] - 10s 42ms/step - loss: 0.3300 - accuracy: 0.8629 - val_loss: 0.4296 - val_accuracy: 0.8090\n",
      "Epoch 4/20\n",
      "247/247 [==============================] - 10s 42ms/step - loss: 0.3045 - accuracy: 0.8713 - val_loss: 0.4479 - val_accuracy: 0.8098\n",
      "Epoch 5/20\n",
      "247/247 [==============================] - 10s 41ms/step - loss: 0.2830 - accuracy: 0.8837 - val_loss: 0.4432 - val_accuracy: 0.8106\n",
      "Epoch 6/20\n",
      "247/247 [==============================] - 10s 42ms/step - loss: 0.2682 - accuracy: 0.8909 - val_loss: 0.4588 - val_accuracy: 0.8128\n",
      "Epoch 7/20\n",
      "247/247 [==============================] - 10s 42ms/step - loss: 0.2499 - accuracy: 0.9010 - val_loss: 0.5237 - val_accuracy: 0.8056\n",
      "Epoch 8/20\n",
      "247/247 [==============================] - 10s 42ms/step - loss: 0.2328 - accuracy: 0.9061 - val_loss: 0.4887 - val_accuracy: 0.8189\n",
      "Epoch 9/20\n",
      "247/247 [==============================] - 11s 43ms/step - loss: 0.2172 - accuracy: 0.9142 - val_loss: 0.5126 - val_accuracy: 0.8125\n",
      "Epoch 10/20\n",
      "247/247 [==============================] - 10s 42ms/step - loss: 0.1977 - accuracy: 0.9184 - val_loss: 0.5821 - val_accuracy: 0.8094\n",
      "Epoch 11/20\n",
      "247/247 [==============================] - 10s 41ms/step - loss: 0.1756 - accuracy: 0.9285 - val_loss: 0.6813 - val_accuracy: 0.7999\n",
      "Epoch 12/20\n",
      "247/247 [==============================] - 10s 42ms/step - loss: 0.1759 - accuracy: 0.9295 - val_loss: 0.6104 - val_accuracy: 0.7976\n",
      "Epoch 13/20\n",
      "247/247 [==============================] - 11s 43ms/step - loss: 0.1546 - accuracy: 0.9380 - val_loss: 0.6589 - val_accuracy: 0.7946\n",
      "Epoch 14/20\n",
      "247/247 [==============================] - 10s 41ms/step - loss: 0.1454 - accuracy: 0.9415 - val_loss: 0.7623 - val_accuracy: 0.7961\n",
      "Epoch 15/20\n",
      "247/247 [==============================] - 10s 41ms/step - loss: 0.1323 - accuracy: 0.9453 - val_loss: 0.8478 - val_accuracy: 0.8022\n",
      "Epoch 16/20\n",
      "247/247 [==============================] - 10s 41ms/step - loss: 0.1217 - accuracy: 0.9508 - val_loss: 0.7958 - val_accuracy: 0.7828\n",
      "Epoch 17/20\n",
      "247/247 [==============================] - 10s 41ms/step - loss: 0.1099 - accuracy: 0.9551 - val_loss: 0.9284 - val_accuracy: 0.7901\n",
      "Epoch 18/20\n",
      "247/247 [==============================] - 10s 41ms/step - loss: 0.1040 - accuracy: 0.9566 - val_loss: 1.0807 - val_accuracy: 0.7851\n",
      "Epoch 19/20\n",
      "247/247 [==============================] - 10s 42ms/step - loss: 0.0995 - accuracy: 0.9598 - val_loss: 0.9859 - val_accuracy: 0.7866\n",
      "Epoch 20/20\n",
      "247/247 [==============================] - 11s 43ms/step - loss: 0.0885 - accuracy: 0.9646 - val_loss: 1.1446 - val_accuracy: 0.7882\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(padded,y_train,epochs=20,validation_data=(padded_val,y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff0d91300876931207232d01add3156fa7c8214350996c757a3c6cebc4b3b5d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
